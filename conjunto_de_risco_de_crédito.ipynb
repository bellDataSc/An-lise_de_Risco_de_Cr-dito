import warnings
warnings.filterwarnings('ignore')
import numpy as np
import pandas as pd
from pathlib import Path
from collections import Counter
from sklearn.metrics import balanced_accuracy_score
from sklearn.metrics import confusion_matrix
from imblearn.metrics import classification_report_imbalanced
Read the CSV and Perform Basic Data Cleaning
# https://help.lendingclub.com/hc/en-us/articles/215488038-What-do-the-different-Note-statuses-mean-

columns = [
    "loan_amnt", "int_rate", "installment", "home_ownership",
    "annual_inc", "verification_status", "issue_d", "loan_status",
    "pymnt_plan", "dti", "delinq_2yrs", "inq_last_6mths",
    "open_acc", "pub_rec", "revol_bal", "total_acc",
    "initial_list_status", "out_prncp", "out_prncp_inv", "total_pymnt",
    "total_pymnt_inv", "total_rec_prncp", "total_rec_int", "total_rec_late_fee",
    "recoveries", "collection_recovery_fee", "last_pymnt_amnt", "next_pymnt_d",
    "collections_12_mths_ex_med", "policy_code", "application_type", "acc_now_delinq",
    "tot_coll_amt", "tot_cur_bal", "open_acc_6m", "open_act_il",
    "open_il_12m", "open_il_24m", "mths_since_rcnt_il", "total_bal_il",
    "il_util", "open_rv_12m", "open_rv_24m", "max_bal_bc",
    "all_util", "total_rev_hi_lim", "inq_fi", "total_cu_tl",
    "inq_last_12m", "acc_open_past_24mths", "avg_cur_bal", "bc_open_to_buy",
    "bc_util", "chargeoff_within_12_mths", "delinq_amnt", "mo_sin_old_il_acct",
    "mo_sin_old_rev_tl_op", "mo_sin_rcnt_rev_tl_op", "mo_sin_rcnt_tl", "mort_acc",
    "mths_since_recent_bc", "mths_since_recent_inq", "num_accts_ever_120_pd", "num_actv_bc_tl",
    "num_actv_rev_tl", "num_bc_sats", "num_bc_tl", "num_il_tl",
    "num_op_rev_tl", "num_rev_accts", "num_rev_tl_bal_gt_0",
    "num_sats", "num_tl_120dpd_2m", "num_tl_30dpd", "num_tl_90g_dpd_24m",
    "num_tl_op_past_12m", "pct_tl_nvr_dlq", "percent_bc_gt_75", "pub_rec_bankruptcies",
    "tax_liens", "tot_hi_cred_lim", "total_bal_ex_mort", "total_bc_limit",
    "total_il_high_credit_limit", "hardship_flag", "debt_settlement_flag"
]

target = ["loan_status"]
# Load the data
file_path = Path('LoanStats_2019Q1.csv')
df = pd.read_csv(file_path, skiprows=1)[:-2]
df = df.loc[:, columns].copy()

# Drop the null columns where all values are null
df = df.dropna(axis='columns', how='all')

# Drop the null rows
df = df.dropna()

# Remove the `Issued` loan status
issued_mask = df['loan_status'] != 'Issued'
df = df.loc[issued_mask]

# convert interest rate to numerical
df['int_rate'] = df['int_rate'].str.replace('%', '')
df['int_rate'] = df['int_rate'].astype('float') / 100


# Convert the target column values to low_risk and high_risk based on their values
x = {'Current': 'low_risk'}   
df = df.replace(x)

x = dict.fromkeys(['Late (31-120 days)', 'Late (16-30 days)', 'Default', 'In Grace Period'], 'high_risk')    
df = df.replace(x)

df.reset_index(inplace=True, drop=True)

df.head()
loan_amnt	int_rate	installment	home_ownership	annual_inc	verification_status	issue_d	loan_status	pymnt_plan	dti	...	pct_tl_nvr_dlq	percent_bc_gt_75	pub_rec_bankruptcies	tax_liens	tot_hi_cred_lim	total_bal_ex_mort	total_bc_limit	total_il_high_credit_limit	hardship_flag	debt_settlement_flag
0	10500.0	0.1719	375.35	RENT	66000.0	Source Verified	Mar-2019	low_risk	n	27.24	...	85.7	100.0	0.0	0.0	65687.0	38199.0	2000.0	61987.0	N	N
1	25000.0	0.2000	929.09	MORTGAGE	105000.0	Verified	Mar-2019	low_risk	n	20.23	...	91.2	50.0	1.0	0.0	271427.0	60641.0	41200.0	49197.0	N	N
2	20000.0	0.2000	529.88	MORTGAGE	56000.0	Verified	Mar-2019	low_risk	n	24.26	...	66.7	50.0	0.0	0.0	60644.0	45684.0	7500.0	43144.0	N	N
3	10000.0	0.1640	353.55	RENT	92000.0	Verified	Mar-2019	low_risk	n	31.44	...	100.0	50.0	1.0	0.0	99506.0	68784.0	19700.0	76506.0	N	N
4	22000.0	0.1474	520.39	MORTGAGE	52000.0	Not Verified	Mar-2019	low_risk	n	18.76	...	100.0	0.0	0.0	0.0	219750.0	25919.0	27600.0	20000.0	N	N
5 rows × 86 columns

Split the Data into Training and Testing
df.select_dtypes(exclude='number').head(4)
# np.unique(df.loan_status)
home_ownership	verification_status	issue_d	loan_status	pymnt_plan	initial_list_status	next_pymnt_d	application_type	hardship_flag	debt_settlement_flag
0	RENT	Source Verified	Mar-2019	low_risk	n	w	May-2019	Individual	N	N
1	MORTGAGE	Verified	Mar-2019	low_risk	n	w	May-2019	Individual	N	N
2	MORTGAGE	Verified	Mar-2019	low_risk	n	w	May-2019	Individual	N	N
3	RENT	Verified	Mar-2019	low_risk	n	w	May-2019	Individual	N	N
# Encode the data from string objects to meaningful numerical forms
df_encoded = pd.get_dummies(df, columns=['home_ownership', 'verification_status', 'pymnt_plan',
                            'initial_list_status', 'application_type', 'hardship_flag', 'debt_settlement_flag'])

# Months dictionary
months_num = {
    "Jan-2019": 1,
    "Feb-2019": 2,
    "Mar-2019": 3,
    "Apr-2019": 4,
    "May-2019": 5
}

# Change the months of issue and next payment to a number corresponding to the month, rather than the string format
df_encoded["issue_d"] = df_encoded["issue_d"].apply(lambda x: months_num[x])
df_encoded["next_pymnt_d"] = df_encoded["next_pymnt_d"].apply(
    lambda x: months_num[x])
# Create our features
# Note that home_ownership, verification_status, issue_date,
X = df_encoded.copy()
X = X.drop(target, axis=1)

# Create our target
y = df_encoded[target]
X.describe()
loan_amnt	int_rate	installment	annual_inc	issue_d	dti	delinq_2yrs	inq_last_6mths	open_acc	pub_rec	...	verification_status_Not Verified	verification_status_Source Verified	verification_status_Verified	pymnt_plan_n	initial_list_status_f	initial_list_status_w	application_type_Individual	application_type_Joint App	hardship_flag_N	debt_settlement_flag_N
count	68817.000000	68817.000000	68817.000000	6.881700e+04	68817.000000	68817.000000	68817.000000	68817.000000	68817.000000	68817.000000	...	68817.000000	68817.000000	68817.000000	68817.0	68817.000000	68817.000000	68817.000000	68817.000000	68817.0	68817.0
mean	16677.594562	0.127718	480.652863	8.821371e+04	1.726172	21.778153	0.217766	0.497697	12.587340	0.126030	...	0.478007	0.373992	0.148001	1.0	0.123879	0.876121	0.860340	0.139660	1.0	1.0
std	10277.348590	0.048130	288.062432	1.155800e+05	0.743862	20.199244	0.718367	0.758122	6.022869	0.336797	...	0.499520	0.483865	0.355104	0.0	0.329446	0.329446	0.346637	0.346637	0.0	0.0
min	1000.000000	0.060000	30.890000	4.000000e+01	1.000000	0.000000	0.000000	0.000000	2.000000	0.000000	...	0.000000	0.000000	0.000000	1.0	0.000000	0.000000	0.000000	0.000000	1.0	1.0
25%	9000.000000	0.088100	265.730000	5.000000e+04	1.000000	13.890000	0.000000	0.000000	8.000000	0.000000	...	0.000000	0.000000	0.000000	1.0	0.000000	1.000000	1.000000	0.000000	1.0	1.0
50%	15000.000000	0.118000	404.560000	7.300000e+04	2.000000	19.760000	0.000000	0.000000	11.000000	0.000000	...	0.000000	0.000000	0.000000	1.0	0.000000	1.000000	1.000000	0.000000	1.0	1.0
75%	24000.000000	0.155700	648.100000	1.040000e+05	2.000000	26.660000	0.000000	1.000000	16.000000	0.000000	...	1.000000	1.000000	0.000000	1.0	0.000000	1.000000	1.000000	0.000000	1.0	1.0
max	40000.000000	0.308400	1676.230000	8.797500e+06	3.000000	999.000000	18.000000	5.000000	72.000000	4.000000	...	1.000000	1.000000	1.000000	1.0	1.000000	1.000000	1.000000	1.000000	1.0	1.0
8 rows × 92 columns

# Check the balance of our target values
y['loan_status'].value_counts()
low_risk     68470
high_risk      347
Name: loan_status, dtype: int64
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=1)
Ensemble Learners
In this section, you will compare two ensemble algorithms to determine which algorithm results in the best performance. You will train a Balanced Random Forest Classifier and an Easy Ensemble AdaBoost classifier . For each algorithm, be sure to complete the folliowing steps:

Train the model using the training data.
Calculate the balanced accuracy score from sklearn.metrics.
Print the confusion matrix from sklearn.metrics.
Generate a classication report using the imbalanced_classification_report from imbalanced-learn.
For the Balanced Random Forest Classifier only, print the feature importance sorted in descending order (most important feature to least important) along with the feature score
Note: Use a random state of 1 for each algorithm to ensure consistency between tests

Balanced Random Forest Classifier
# Create the Balanced Random Forest Classifier
from imblearn.ensemble import BalancedRandomForestClassifier
barf = BalancedRandomForestClassifier(n_estimators=100, random_state=1)
# Fit the model
barf.fit(X_train, y_train)
BalancedRandomForestClassifier(random_state=1)
# Calculated the balanced accuracy score
y_pred = barf.predict(X_test)
balanced_accuracy_score(y_test, y_pred)
0.7702781101562783
# Display the confusion matrix
confusion_matrix(y_test, y_pred)
array([[   57,    30],
       [ 1962, 15156]], dtype=int64)
# Print the imbalanced classification report
print(classification_report_imbalanced(y_test, y_pred))
                   pre       rec       spe        f1       geo       iba       sup

  high_risk       0.03      0.66      0.89      0.05      0.76      0.57        87
   low_risk       1.00      0.89      0.66      0.94      0.76      0.59     17118

avg / total       0.99      0.88      0.66      0.93      0.76      0.59     17205

# List the features sorted in descending order by feature importance
sorted(zip(barf.feature_importances_,X.columns), reverse=True)
[(0.08616090612158625, 'total_rec_prncp'),
 (0.06819804992381384, 'last_pymnt_amnt'),
 (0.05502788156106545, 'total_rec_int'),
 (0.054927686320484025, 'total_pymnt'),
 (0.04504531165501469, 'total_pymnt_inv'),
 (0.029786571004434898, 'int_rate'),
 (0.02604666383241835, 'issue_d'),
 (0.02104211652991394, 'out_prncp'),
 (0.01984328076654203, 'installment'),
 (0.018717192890254232, 'dti'),
 (0.017585211416815086, 'mths_since_recent_inq'),
 (0.017200018972252793, 'total_rev_hi_lim'),
 (0.01609091902674421, 'annual_inc'),
 (0.01581535690702728, 'avg_cur_bal'),
 (0.015586646933556794, 'mo_sin_old_il_acct'),
 (0.015260027518257037, 'bc_util'),
 (0.015125853458147813, 'max_bal_bc'),
 (0.01510239936294808, 'total_bal_ex_mort'),
 (0.014572963819907228, 'revol_bal'),
 (0.014180623052976871, 'mo_sin_old_rev_tl_op'),
 (0.013606707940860418, 'total_bc_limit'),
 (0.013575040418237807, 'total_il_high_credit_limit'),
 (0.013152458734595042, 'mths_since_rcnt_il'),
 (0.012827086604106136, 'loan_amnt'),
 (0.012806753954872472, 'mths_since_recent_bc'),
 (0.01277951292019627, 'il_util'),
 (0.01270923841361903, 'total_bal_il'),
 (0.012598842620124137, 'out_prncp_inv'),
 (0.012588553016772057, 'bc_open_to_buy'),
 (0.012391652223720406, 'all_util'),
 (0.012229002169250516, 'tot_cur_bal'),
 (0.012148675324690217, 'mo_sin_rcnt_rev_tl_op'),
 (0.01181401312877912, 'tot_hi_cred_lim'),
 (0.011743569706989199, 'num_sats'),
 (0.011258268395036403, 'num_rev_accts'),
 (0.01094116783287314, 'total_acc'),
 (0.010806079589928871, 'num_actv_rev_tl'),
 (0.010582228595594092, 'num_il_tl'),
 (0.009034679674255535, 'acc_open_past_24mths'),
 (0.00896721872911922, 'num_bc_tl'),
 (0.00895693383857899, 'mo_sin_rcnt_tl'),
 (0.008797801093451975, 'open_acc'),
 (0.008542756327497117, 'pct_tl_nvr_dlq'),
 (0.00835940221574824, 'num_actv_bc_tl'),
 (0.008179198342145916, 'num_op_rev_tl'),
 (0.007998369173311856, 'num_rev_tl_bal_gt_0'),
 (0.007632717952947504, 'inq_last_12m'),
 (0.0073860007778242115, 'num_bc_sats'),
 (0.007008395210516902, 'total_cu_tl'),
 (0.006916302733294581, 'open_il_24m'),
 (0.0065913413196325875, 'mort_acc'),
 (0.006585430532657023, 'num_tl_op_past_12m'),
 (0.0062615229487147745, 'inq_last_6mths'),
 (0.006227834160712324, 'percent_bc_gt_75'),
 (0.006141870583987469, 'inq_fi'),
 (0.005916884297705603, 'open_rv_24m'),
 (0.0058636823314487324, 'total_rec_late_fee'),
 (0.0058371068994518644, 'open_act_il'),
 (0.004953603091805592, 'next_pymnt_d'),
 (0.004861569295015042, 'open_rv_12m'),
 (0.004808348572648847, 'open_il_12m'),
 (0.004335588888978948, 'num_accts_ever_120_pd'),
 (0.004250179196399507, 'open_acc_6m'),
 (0.003980747767303762, 'tot_coll_amt'),
 (0.0030699719600632163, 'verification_status_Verified'),
 (0.002719641829606877, 'delinq_2yrs'),
 (0.0021047911743905068, 'pub_rec_bankruptcies'),
 (0.0020546954071233724, 'home_ownership_RENT'),
 (0.001872734605414072, 'verification_status_Not Verified'),
 (0.0018276473093019896, 'pub_rec'),
 (0.001403835805032324, 'home_ownership_MORTGAGE'),
 (0.001309787473811562, 'home_ownership_OWN'),
 (0.0012816836211147749, 'application_type_Joint App'),
 (0.0012519454175474864, 'verification_status_Source Verified'),
 (0.0011722554847900582, 'initial_list_status_f'),
 (0.0011567914322937443, 'initial_list_status_w'),
 (0.0008629882642490638, 'application_type_Individual'),
 (0.0007766884196574949, 'num_tl_90g_dpd_24m'),
 (0.00044654220339788585, 'home_ownership_ANY'),
 (0.00035323090789528906, 'chargeoff_within_12_mths'),
 (6.475006475006477e-05, 'collections_12_mths_ex_med'),
 (0.0, 'tax_liens'),
 (0.0, 'recoveries'),
 (0.0, 'pymnt_plan_n'),
 (0.0, 'policy_code'),
 (0.0, 'num_tl_30dpd'),
 (0.0, 'num_tl_120dpd_2m'),
 (0.0, 'hardship_flag_N'),
 (0.0, 'delinq_amnt'),
 (0.0, 'debt_settlement_flag_N'),
 (0.0, 'collection_recovery_fee'),
 (0.0, 'acc_now_delinq')]
Easy Ensemble AdaBoost Classifier
# Train the EasyEnsembleClassifier
from imblearn.ensemble import EasyEnsembleClassifier
easy_e = EasyEnsembleClassifier(n_estimators=100, random_state=1)
easy_e.fit(X_train, y_train)
EasyEnsembleClassifier(n_estimators=100, random_state=1)
# Calculated the balanced accuracy score
y_pred = easy_e.predict(X_test)
balanced_accuracy_score(y_test, y_pred)
0.9254565671948463
# Display the confusion matrix
confusion_matrix(y_test, y_pred)
array([[   79,     8],
       [  978, 16140]], dtype=int64)
# Print the imbalanced classification report
print(classification_report_imbalanced(y_test, y_pred))
                   pre       rec       spe        f1       geo       iba       sup

  high_risk       0.07      0.91      0.94      0.14      0.93      0.85        87
   low_risk       1.00      0.94      0.91      0.97      0.93      0.86     17118

avg / total       0.99      0.94      0.91      0.97      0.93      0.86     17205
